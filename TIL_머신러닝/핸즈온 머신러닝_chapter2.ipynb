{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 누락된 값 다루기 - SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 설정\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "\n",
    "# 훈련\n",
    "imputer.fit()\n",
    "\n",
    "# 각 특성 별 관련 값 반환\n",
    "imputer.statistics_\n",
    "df.median().values\n",
    "\n",
    "# 누락된 값에 채우기\n",
    "X = imputer.transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트와 범주형 특성 다루기\n",
    "- OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder # 라벨 인코딩\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df명_ = ordinal_encoder.fit_transform(df명) \n",
    "\n",
    "df명_.categories_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 2.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new]\n",
    "X_new_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- y = *h*θ(x) = **θ**·**x**\n",
    "\n",
    "**θ**·**x의 첫 벡터는 편향 =**  *θ*0*x*0\n",
    "\n",
    "\n",
    "\n",
    "## **The Normal Equation - 정규방정식**\n",
    "\n",
    "비용함수를 최소화하는 $\\theta$값을 찾기위한 해석적인 방법\n",
    "\n",
    "$\\hat\\theta=(X^TX)^{-1}X^Ty$\n",
    "\n",
    "\n",
    "## 가우시안 노이즈(Gaussian Noise)\n",
    "- 정규분포를 가지는 잡음. 쉽게 말해서 일반적인 잡음이며 어느정도 랜덤하면서 자연계에서 쉽게 볼 수  있는 분포를 말한다.\n",
    "\n",
    "\n",
    "the *pseudoinverse -* 유사역행렬\n",
    "\n",
    "[선형대수학](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99)에서 **무어-펜로즈 유사역행렬**(Moore-Penrose疑似逆行列, [영어](https://ko.wikipedia.org/wiki/%EC%98%81%EC%96%B4): Moore–Penrose pseudoinverse matrix)은 모든 모양의 행렬에 대하여 정의되는 연산이며, [가역 행렬](https://ko.wikipedia.org/wiki/%EA%B0%80%EC%97%AD_%ED%96%89%EB%A0%AC)의 [역행렬](https://ko.wikipedia.org/wiki/%EC%97%AD%ED%96%89%EB%A0%AC) 연산을 일반화한다.[[1]](https://ko.wikipedia.org/wiki/%EB%AC%B4%EC%96%B4-%ED%8E%9C%EB%A1%9C%EC%A6%88_%EC%9C%A0%EC%82%AC%EC%97%AD%ED%96%89%EB%A0%AC#cite_note-1):제8장[[2]](https://ko.wikipedia.org/wiki/%EB%AC%B4%EC%96%B4-%ED%8E%9C%EB%A1%9C%EC%A6%88_%EC%9C%A0%EC%82%AC%EC%97%AD%ED%96%89%EB%A0%AC#cite_note-2):제13장[[3]](https://ko.wikipedia.org/wiki/%EB%AC%B4%EC%96%B4-%ED%8E%9C%EB%A1%9C%EC%A6%88_%EC%9C%A0%EC%82%AC%EC%97%AD%ED%96%89%EB%A0%AC#cite_note-3):제6장 [특잇값 분해](https://ko.wikipedia.org/wiki/%ED%8A%B9%EC%9E%87%EA%B0%92_%EB%B6%84%ED%95%B4)를 통해 계산할 수 있다.\n",
    "\n",
    "- 사용하는 이유 : 정규방정식보다 효율적. 실제로 m<n이거나 어떤 특성이 중복되어 행렬 $X^TX$의 역행렬이 없다면(즉, 특이행렬이라면) 정규방정식이 작동하지 않습니다. 하지만 유사역행렬은 항상 구할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent(경사하강법)\n",
    "\n",
    "임의의 파라미터값에서 부터 시작\n",
    "\n",
    "learning rate를 통해 얼마만큼 이동할지를 결정\n",
    "\n",
    "global minimum(전역 최솟값)\n",
    "\n",
    "local minimum(지역 최솟값)\n",
    "\n",
    "경사하강법은 learning rate에 따라 전역 최솟값을 못찾을 수 있음\n",
    "\n",
    "다만 MSE 비용함수는 이차함수로서 어떤 두점을 선택해 선을 그어도 곡선을 가로지르지 않는 볼록함수(convex function) - 전역 최솟값만 존재\n",
    "\n",
    "특성간 스케일 차이에 따른 성능차이가 발생한다. 경사하강법을 사용시 스케일링 필요\n",
    "\n",
    "parameter space\n",
    "- 특성간에 파라미터를 찾는 공간\n",
    "\n",
    "### **Batch Gradient Descent (배치 경사 하강법)**\n",
    "\n",
    "partial derivative(편도함수)\n",
    "- 두 개의 변수 x,y의 함수 f(x,y)가 주어진 경우 f(x,y)에서 y를 상수로 보고 얻어지는 편미분계수(偏微分係數)와 x를 상수로 보고서 얻어지는 편미분계수를 다시 x와 y의 함수로 보았을 때, 이들을 각각 x에 관한 제1차 편도함수, y에 관한 제1차 편도함수라 한다.\n",
    "\n",
    "배치\n",
    "- GPU(병렬처리)가 한번에 처리하는 데이터의 묶음\n",
    "- `전체학습 데이터를 하나의 배치로 묶어 학습`시키는 경사하강법(각 파라미터의 오차값을 한번에 반영한다.)\n",
    "\n",
    "gradient vector of the cost function(비용함수의 그레디언트 벡터)\n",
    "\n",
    "tolerance(허용오차)\n",
    "- 최적의 파라미터를 찾기위해 그리드탐색에서 반복횟수를 지정하는 방법은 반복횟수를 아주 크게지정하고 그레디언트벡터가 일정 허용오차보다 작아지면 알고리즘을 중지하는 것을 이용한다.\n",
    "\n",
    "### Stochastic Gradient Descent (확률적 경사 하강법)\n",
    "\n",
    "무작위 선택 - `하나의 샘플을 기반`로 그레디언트 계산\n",
    "\n",
    "샘플을 무작위로 선택하기에 불안정하다.\n",
    "\n",
    "최적화가 지역최저점에 빠질 확률을 줄여준다. - 무작위 선택으로 인해 지역최솟값을 건너뛰도록 도와준다.\n",
    "→ 최적치는 아님. 전역 최솟값에 다다르지 못하게 한다.\n",
    "\n",
    "*해결 방법*\n",
    "→ *simulated annealing(담금질 기법)* 사용\n",
    "- 학습률을 점진적으로 감소시키는 것 방법이다. 처음에는 큰 learning rate를 사용하다 목적값에 다다를 수록 작은 학습률을 사용\n",
    "- learning schedule(학습스케쥴)\n",
    "    - 매 반복에서 학습률을 결정하는 함수\n",
    "    - 학습률을 Epoch마다 작게 변환시킨다.\n",
    "\n",
    "IID(independent and identically distributed)\n",
    "- 에포크마다 모든 샘플을 사용하게 하려면 훈련하는 동안 샘플을 섞는 것이 필요\n",
    "- 섞지않을 시 동일한 샘플을 사용할 수 있다.\n",
    "\n",
    "mini-batch gradient descent(미니배치 경사 하강법)\n",
    "- GPU를 사용해 `임의의 작은 샘플 세트`(미니배치)에 대해 그레디언트를 계산\n",
    "- 덜 불규칙 . 각 배치별 평균값으로 이동 → 지역최솟값에서 빠져나오기는 힘들 수 있음\n",
    "\n",
    "*theta = 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35aa464e4f518e3bd80b827b0a7a82608bf016d4c837622545242b48b9bfee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
