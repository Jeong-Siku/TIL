{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3bc28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/14 04:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MAX_MEMORY=\"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediciton\")\\\n",
    "                .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "                .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9ff75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/ubuntu/working/spark-examples/data/ml-data\"\n",
    "\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c12a41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ed0f396fb699,\n",
       " OneHotEncoder_e2fb91c14a06,\n",
       " StringIndexer_b2ae5c1fb936,\n",
       " OneHotEncoder_b84305047842,\n",
       " StringIndexer_8a90891a4bf7,\n",
       " OneHotEncoder_f1990d4b5772]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# OneHotEncoding을 수행할 컬럼을 지정\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    # 1. 데이터를 문자열 형식으로 바꿔준다. setHandleInvalid : Null값 같은 데이터를 어떻게 처리 할건지\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    # 2. One Hot Encoding 수행\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c+\"_onehot\"])\n",
    "    \n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa76e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ed0f396fb699,\n",
       " OneHotEncoder_e2fb91c14a06,\n",
       " StringIndexer_b2ae5c1fb936,\n",
       " OneHotEncoder_b84305047842,\n",
       " StringIndexer_8a90891a4bf7,\n",
       " OneHotEncoder_f1990d4b5772,\n",
       " VectorAssembler_ee4490e19a21,\n",
       " StandardScaler_0e20464a069c,\n",
       " VectorAssembler_bed5b2027e56,\n",
       " StandardScaler_6ceb27b76a8f,\n",
       " VectorAssembler_5b8991ab86e7,\n",
       " StandardScaler_3eb6335109e2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    \n",
    "    # 각각의 컬럼의 데이터가 벡터화. ex) 1.5 -> [1.5]\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    \n",
    "    # StandardScaling 수행\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "    \n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63116f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ed0f396fb699,\n",
       " OneHotEncoder_e2fb91c14a06,\n",
       " StringIndexer_b2ae5c1fb936,\n",
       " OneHotEncoder_b84305047842,\n",
       " StringIndexer_8a90891a4bf7,\n",
       " OneHotEncoder_f1990d4b5772,\n",
       " VectorAssembler_ee4490e19a21,\n",
       " StandardScaler_0e20464a069c,\n",
       " VectorAssembler_bed5b2027e56,\n",
       " StandardScaler_6ceb27b76a8f,\n",
       " VectorAssembler_5b8991ab86e7,\n",
       " StandardScaler_3eb6335109e2,\n",
       " VectorAssembler_28a5d8327619]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _onehot이 붙은 컬럼과 _scaled 가 붙은 컬럼만 있으면 된다.\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_features] + [n + \"_scaled\" for n in num_features]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b603ad",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ed2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression # Ridge, Lasso가 없고, ElasticNet을 포함\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff92f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "lr = LinearRegression(\n",
    "    maxIter= 30,\n",
    "    solver='normal',\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"feature_vector\")\n",
    "\n",
    "# LinearRegression 모델까지 하나의 파이프라인으로 통합\n",
    "cv_stages = stages+[lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9fffeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 생성\n",
    "cv_pipeline = Pipeline(stages = cv_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e534eea",
   "metadata": {},
   "source": [
    " ## GridSearch 및 CrossValidation 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440c25aa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.02},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.03},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.04},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.02},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.03},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.04},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.3,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.3,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.02},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.3,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.03},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.3,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.04},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.3,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.02},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.03},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.04},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.02},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.03},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.04},\n",
       " {Param(parent='LinearRegression_72c58de5e02a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5,\n",
       "  Param(parent='LinearRegression_72c58de5e02a', name='regParam', doc='regularization parameter (>= 0).'): 0.05}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = ParamGridBuilder()\\\n",
    "            .addGrid(lr.elasticNetParam, [0.1, 0.2, 0.3, 0.4, 0.5])\\ # 혼합율 조절\n",
    "            .addGrid(lr.regParam, [0.01, 0.02, 0.03, 0.04, 0.05])\\ # 정규화 조절\n",
    "            .build()\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "034ec5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_877ec19dde50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val = CrossValidator(\n",
    "        estimator=cv_pipeline, # 파이프라인을 Estimator로 넣는 경우 파이프라인에 마지막 stage는 꼭 모델이어야 한다.\n",
    "        estimatorParamMaps=param_grid, # 없으면 그냥 GridSearch없이 Cross Validation만 진행\n",
    "        evaluator=RegressionEvaluator(labelCol=\"total_amount\"),\n",
    "        numFolds=5)\n",
    "    \n",
    "cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33528135",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f946e829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플 데이터 세트 만들기. 전체로 다 하면 시간이 오래 걸린다.\n",
    "toy_df = train_df.sample(False, 0.1, seed=1) # withReplacement : 복원추출 여부, fraction\n",
    "toy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91e0276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 04:22:42 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/06/14 04:22:42 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/06/14 04:22:44 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/06/14 04:22:44 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv_model = cross_val.fit(toy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b093a",
   "metadata": {},
   "source": [
    "## BestModel 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c5cd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256d7f1",
   "metadata": {},
   "source": [
    "## Best Parameter 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaea9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam() # 모델까지 다 들어가 있는 list에서 제일 마지막(cv_stages와 동일)\n",
    "best_reg_param = cv_model.bestModel.stages[-1]._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a8110b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 0.2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_alpha, best_reg_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3ce7b",
   "metadata": {},
   "source": [
    "# 전체 데이터를 대상으로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7332316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages) # 모델이 빠진 전처리만 하는 파이프라인 생성\n",
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a1452ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe175009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameter로 모델 생성하기\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver=\"normal\",\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"feature_vector\",\n",
    "    elasticNetParam=best_alpha,\n",
    "    regParam=best_reg_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0abe667",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jvm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec_train_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/wrapper.py:339\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 339\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_java\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    Fits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m        fitted Java model\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transfer_params_to_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/wrapper.py:150\u001b[0m, in \u001b[0;36mJavaParams._transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair_defaults) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    149\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m--> 150\u001b[0m     pair_defaults_seq \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(pair_defaults)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39msetDefault(pair_defaults_seq)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vec_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601763cf",
   "metadata": {},
   "source": [
    "# 튜닝된 모델 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92871f76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jvm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델이 저장될 디렉토리 지정\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/working/spark-examples/taxi_pricing_model1/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/util.py:226\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/util.py:248\u001b[0m, in \u001b[0;36mGeneralJavaMLWritable.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an GeneralMLWriter instance for this ML instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneralJavaMLWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/util.py:201\u001b[0m, in \u001b[0;36mGeneralJavaMLWriter.__init__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGeneralJavaMLWriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/util.py:170\u001b[0m, in \u001b[0;36mJavaMLWriter.__init__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28msuper\u001b[39m(JavaMLWriter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 170\u001b[0m     _java_obj \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite \u001b[38;5;241m=\u001b[39m _java_obj\u001b[38;5;241m.\u001b[39mwrite()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/wrapper.py:230\u001b[0m, in \u001b[0;36mJavaParams._to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_java\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Transfer this instance's Params to the wrapped Java object, and return the Java object.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    Used for ML persistence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        Java object equivalent to this instance.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transfer_params_to_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/wrapper.py:150\u001b[0m, in \u001b[0;36mJavaParams._transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair_defaults) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    149\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m--> 150\u001b[0m     pair_defaults_seq \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(pair_defaults)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39msetDefault(pair_defaults_seq)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
     ]
    }
   ],
   "source": [
    "# 모델이 저장될 디렉토리 지정\n",
    "model_dir = \"/home/ubuntu/working/spark-examples/taxi_pricing_model1/\"\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로딩\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "loaded_model = LinearRegression().load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b89fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60f9d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7a604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6c723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7590d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e35aa464e4f518e3bd80b827b0a7a82608bf016d4c837622545242b48b9bfee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
