{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea80dfae",
   "metadata": {},
   "source": [
    "# 카탈리스트\n",
    "- 쿼리에 대한 논리적인 실행 계획을 수립\n",
    "- 쿼리 분석 -> 논리적 계획 수립 -> 물리적 계획 수립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053599eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/07 04:08:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d788a",
   "metadata": {},
   "source": [
    "## CSV를 불러와서 Spark DF로 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33c507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath=\"/home/ubuntu/working/spark-examples/data/titanic_train.csv\"\n",
    "titanic_sdf = spark.read.csv(filepath,\n",
    "                            inferSchema=True, # 데이터의 타입을 스파크가 자동으로 인식\n",
    "                            header=True, # 첫 줄을 불러올지 말지 결정\n",
    "                            )\n",
    "titanic_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d3a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf.createOrReplaceTempView(\"titanic\") # 임시view 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976207ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select *\n",
    "from titanic\n",
    "limit 5\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7566b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+\n",
      "|Embarked|Pclass|male_cnt|\n",
      "+--------+------+--------+\n",
      "|       C|     1|      42|\n",
      "|       C|     2|      10|\n",
      "|       C|     3|      43|\n",
      "|       Q|     1|       1|\n",
      "|       Q|     2|       1|\n",
      "|       Q|     3|      39|\n",
      "|       S|     1|      79|\n",
      "|       S|     2|      97|\n",
      "|       S|     3|     265|\n",
      "+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 남자 중 Embarked(탑승지) 별 Pclass(좌석 등급) 마다 몇 명이 탔는지\n",
    "query = \"\"\"\n",
    "select Embarked, Pclass, count(*) as male_cnt\n",
    "from titanic\n",
    "where Sex = \"male\"\n",
    "group by Embarked, Pclass\n",
    "order by Embarked, Pclass\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cdd5494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Embarked ASC NULLS FIRST, 'Pclass ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['Embarked, 'Pclass], ['Embarked, 'Pclass, 'count(1) AS male_cnt#268]\n",
      "   +- 'Filter ('Sex = male)\n",
      "      +- 'UnresolvedRelation [titanic], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Embarked: string, Pclass: int, male_cnt: bigint\n",
      "Sort [Embarked#67 ASC NULLS FIRST, Pclass#58 ASC NULLS FIRST], true\n",
      "+- Aggregate [Embarked#67, Pclass#58], [Embarked#67, Pclass#58, count(1) AS male_cnt#268L]\n",
      "   +- Filter (Sex#60 = male)\n",
      "      +- SubqueryAlias titanic\n",
      "         +- View (`titanic`, [PassengerId#56,Survived#57,Pclass#58,Name#59,Sex#60,Age#61,SibSp#62,Parch#63,Ticket#64,Fare#65,Cabin#66,Embarked#67])\n",
      "            +- Relation [PassengerId#56,Survived#57,Pclass#58,Name#59,Sex#60,Age#61,SibSp#62,Parch#63,Ticket#64,Fare#65,Cabin#66,Embarked#67] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Embarked#67 ASC NULLS FIRST, Pclass#58 ASC NULLS FIRST], true\n",
      "+- Aggregate [Embarked#67, Pclass#58], [Embarked#67, Pclass#58, count(1) AS male_cnt#268L]\n",
      "   +- Project [Pclass#58, Embarked#67]\n",
      "      +- Filter (isnotnull(Sex#60) AND (Sex#60 = male))\n",
      "         +- Relation [PassengerId#56,Survived#57,Pclass#58,Name#59,Sex#60,Age#61,SibSp#62,Parch#63,Ticket#64,Fare#65,Cabin#66,Embarked#67] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Embarked#67 ASC NULLS FIRST, Pclass#58 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Embarked#67 ASC NULLS FIRST, Pclass#58 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=240]\n",
      "      +- HashAggregate(keys=[Embarked#67, Pclass#58], functions=[count(1)], output=[Embarked#67, Pclass#58, male_cnt#268L])\n",
      "         +- Exchange hashpartitioning(Embarked#67, Pclass#58, 200), ENSURE_REQUIREMENTS, [plan_id=237]\n",
      "            +- HashAggregate(keys=[Embarked#67, Pclass#58], functions=[partial_count(1)], output=[Embarked#67, Pclass#58, count#274L])\n",
      "               +- Project [Pclass#58, Embarked#67]\n",
      "                  +- Filter (isnotnull(Sex#60) AND (Sex#60 = male))\n",
      "                     +- FileScan csv [Pclass#58,Sex#60,Embarked#67] Batched: false, DataFilters: [isnotnull(Sex#60), (Sex#60 = male)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/titanic_train.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Sex), EqualTo(Sex,male)], ReadSchema: struct<Pclass:int,Sex:string,Embarked:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True) # 아래에서 위로 읽을 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c450d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d8daa",
   "metadata": {},
   "source": [
    "택시 데이터에 대한 다음 쿼리의 실행 계획을 분석. 테이블 이름은 mobility_data\n",
    "\n",
    "select   \n",
    "     pickup_date,   \n",
    "    count(*) as trips  \n",
    "from (  select  \n",
    "            split(pickup_datetime, ' ')[0] as pickup_date  \n",
    "        from mobility_data )  \n",
    "group by pickup_date  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3d91896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"mobility_data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c81decce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv\"\n",
    "taxi_sdf = spark.read.csv(filepath,\n",
    "                            inferSchema=True, # 데이터의 타입을 스파크가 자동으로 인식\n",
    "                            header=True, # 첫 줄을 불러올지 말지 결정\n",
    "                            )\n",
    "taxi_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "041a9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sdf.createOrReplaceTempView(\"mobility_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0483a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['pickup_date], ['pickup_date, 'count(1) AS trips#349]\n",
      "+- 'SubqueryAlias __auto_generated_subquery_name\n",
      "   +- 'Project ['split('pickup_datetime,  )[0] AS pickup_date#348]\n",
      "      +- 'UnresolvedRelation [mobility_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pickup_date: string, trips: bigint\n",
      "Aggregate [pickup_date#348], [pickup_date#348, count(1) AS trips#349L]\n",
      "+- SubqueryAlias __auto_generated_subquery_name\n",
      "   +- Project [split(pickup_datetime#293,  , -1)[0] AS pickup_date#348]\n",
      "      +- SubqueryAlias mobility_data\n",
      "         +- View (`mobility_data`, [hvfhs_license_num#291,dispatching_base_num#292,pickup_datetime#293,dropoff_datetime#294,PULocationID#295,DOLocationID#296,SR_Flag#297])\n",
      "            +- Relation [hvfhs_license_num#291,dispatching_base_num#292,pickup_datetime#293,dropoff_datetime#294,PULocationID#295,DOLocationID#296,SR_Flag#297] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [pickup_date#348], [pickup_date#348, count(1) AS trips#349L]\n",
      "+- Project [split(pickup_datetime#293,  , -1)[0] AS pickup_date#348]\n",
      "   +- Relation [hvfhs_license_num#291,dispatching_base_num#292,pickup_datetime#293,dropoff_datetime#294,PULocationID#295,DOLocationID#296,SR_Flag#297] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[pickup_date#348], functions=[count(1)], output=[pickup_date#348, trips#349L])\n",
      "   +- Exchange hashpartitioning(pickup_date#348, 200), ENSURE_REQUIREMENTS, [plan_id=304]\n",
      "      +- HashAggregate(keys=[pickup_date#348], functions=[partial_count(1)], output=[pickup_date#348, count#354L])\n",
      "         +- Project [split(pickup_datetime#293,  , -1)[0] AS pickup_date#348]\n",
      "            +- FileScan csv [pickup_datetime#293] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pickup_datetime:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query='''\n",
    "select   \n",
    "    pickup_date,   \n",
    "    count(*) as trips  \n",
    "from (  select  \n",
    "            split(pickup_datetime, ' ')[0] as pickup_date  \n",
    "        from mobility_data )  \n",
    "group by pickup_date \n",
    "\n",
    "'''\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667656f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e35aa464e4f518e3bd80b827b0a7a82608bf016d4c837622545242b48b9bfee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
