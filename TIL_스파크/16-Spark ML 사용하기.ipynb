{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008363fe",
   "metadata": {},
   "source": [
    "# 사이킷런 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4024393",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a4e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf658eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0             5.1          3.5           1.4          0.2       0\n",
       "1             4.9          3.0           1.4          0.2       0\n",
       "2             4.7          3.2           1.3          0.2       0\n",
       "3             4.6          3.1           1.5          0.2       0\n",
       "4             5.0          3.6           1.4          0.2       0\n",
       "..            ...          ...           ...          ...     ...\n",
       "145           6.7          3.0           5.2          2.3       2\n",
       "146           6.3          2.5           5.0          1.9       2\n",
       "147           6.5          3.0           5.2          2.0       2\n",
       "148           6.2          3.4           5.4          2.3       2\n",
       "149           5.9          3.0           5.1          1.8       2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris datasets 로딩\n",
    "iris = load_iris()\n",
    "\n",
    "iris_data  = iris.data # feature\n",
    "iris_label = iris.target # label\n",
    "\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "iris_pdf = pd.DataFrame(iris_data, columns=iris_columns)\n",
    "iris_pdf['target'] = iris_label\n",
    "iris_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8162218",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pdf.to_csv(\"./data/iris.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23072a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할 및 모델 생성\n",
    "# from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier # Estimator\n",
    "from sklearn.model_selection import train_test_split # RandomSpliter\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(\n",
    "    iris_data,\n",
    "    iris_label,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, t_train) # 훈련! tree_clf 모델 자체에서 훈련이 일어나게 된다.\n",
    "\n",
    "pred = tree_clf.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61bd3a",
   "metadata": {},
   "source": [
    "# Spark ML 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5238af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/13 04:17:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"tree-clf\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3974d130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|     0|\n",
      "|         4.9|        3.0|         1.4|        0.2|     0|\n",
      "|         4.7|        3.2|         1.3|        0.2|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "|         5.0|        3.6|         1.4|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_filepath = \"/home/ubuntu/working/spark-examples/data/iris.csv\"\n",
    "iris_sdf = spark.read.csv(f\"file://{iris_filepath}\",inferSchema=True, header= True)\n",
    "iris_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a9d7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomSplit 메소드를 활용해 훈련 / 테스트 데이터 세트 분할\n",
    "train_sdf, test_sdf = iris_sdf.randomSplit([0.8,0.2],seed =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f509829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 세트는 어떻게 변환이 되어도 하나만 존재하는게 좋다!\n",
    "# -> 모델을 여러 개 사용해서 변환이 되는 상황\n",
    "# 훈련 데이터가 모델에 들어감녀 transform이 일어나게 된다.\n",
    "# 여러 번의 훈련을 거치게 되면 transform이 여러 번 일어나게 된다.\n",
    "#   -> train_sdsf가 메모리 내에 여러 개가 똑같은 것이 생길 수 있다.\n",
    "\n",
    "# 훈련 직전에 사용할 데이터는 캐싱을 하는게 좋다.\n",
    "# RDD의 특징 상 동일한 데이터를 반복해서 가져오는 것은 비효율\n",
    "# 훈련을 할 때 훈련데이터는 똑같기 때문에 cache,persist를 통해 메모리에 저장해 놓는다.\n",
    "train_sdf.cache()\n",
    "train_sdf.show(5)\n",
    "# 로우는 각 컬럼 데이터가 떨어져 있는 상황 : 벡터 어셈블러가 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95ab5d",
   "metadata": {},
   "source": [
    "`VectorAssembler`를 이용하여 모든 feature 컬럼을 하나의 feature vector로 만든다.(행 벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e2797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|[4.3,3.0,1.1,0.1]|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|[4.4,2.9,1.4,0.2]|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|[4.4,3.2,1.3,0.2]|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|[4.5,2.3,1.3,0.3]|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|[4.6,3.1,1.5,0.2]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 합쳐질 컬럼 목록\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# VectorAssembler로 데이터프레임에 있는 데이터를 하나의 행벡터로 합쳐준다.\n",
    "\n",
    "# inputCols : 합쳐질 컬럼들의 목록\n",
    "# outputCol : 합쳐진 컬럼의 이름\n",
    "vec_assembler = VectorAssembler(inputCols=iris_columns,outputCol=\"features\") # 별도로 어셈블러할 데이터프레임을 작성하지않아도 됨.\n",
    "\n",
    "# VectorAssembler Transform\n",
    "train_feature_vector_sdf = vec_assembler.transform(train_sdf)\n",
    "train_feature_vector_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2507dc4",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "Spark ML의 모델은 추정기(Estimator)지만, 데이터를 변환시키는 Transformer에 해당한다.\n",
    "- train 데이터를 받아서 예측 값(prediction)으로 변환시키는 transform 과정이 일어나기 때문!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "030fcea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.DecisionTreeClassifier"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# 모델 생성. 어떤 컬럼의 데이터를 이용해서 학습할지 결정을 지어줘야 한다.\n",
    "# 데이터프레임 기반이기 때문에 컬럼 정의가 필수!\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"target\",\n",
    "    maxDepth=5\n",
    ")\n",
    "type(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efad7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f16c84d71f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/spark_env/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 39, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'DecisionTreeClassifier' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.DecisionTreeClassificationModel"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습. fit() 메소드를 이용하여 학습을 수행하고, 그 결과를 ML 모델로 변환한다.\n",
    "dt_model = dt.fit(train_feature_vector_sdf)\n",
    "type(dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17a9ca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 예측\n",
    "test_sdf.show(5)\n",
    "\n",
    "# 훈련 데이터에서 적용시켰던 Transformer를 테스트 세트에도 그대로 적용시킨다.\n",
    "test_feature_vector_sdf = vec_assembler.transform(test_sdf)\n",
    "test_feature_vector_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e39201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|  probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측 : transform 사용\n",
    "# 기존 값에 예측값을 덧붙인다.\n",
    "predictions = dt_model.transform(test_feature_vector_sdf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cc89a6c",
   "metadata": {},
   "source": [
    "* `rawPrediction` : 머신러닝 모델 알고리즘 별로 다를 수 있다.\n",
    "    * 머신러닝 알고리즘에 의해서 계산된 값\n",
    "    * 값에 대한 정확한 의미는 없다.\n",
    "    * `LogisticRegression`의 경우 예측 label 별로, 예측 수행 전 `sigmoid` 함수 적용 전 값\n",
    "        * $ \\hat{y} = \\sigma(WX + b) $\n",
    "        * $ WX + b $의 결과가 `rawPrediction`\n",
    "* `probability` : 예측 label 별 예측 확률 값\n",
    "* `prediction` : 최종 예측 label 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1aba7",
   "metadata": {},
   "source": [
    "# 모델평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cdb0622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        labelCol = \"target\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a20db061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|       rawPrediction|         probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[18.6086266693526...|[0.99997762791224...|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[18.8180066107263...|[0.99997581287298...|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[22.6963845270307...|[0.99999942608846...|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[16.7506644665745...|[0.99971232954776...|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[17.3393987944099...|[0.99969323067671...|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogistciRegression 사용해보기[실습]\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "   featuresCol=\"features\",\n",
    "    labelCol=\"target\",\n",
    "    maxIter= 10)\n",
    "\n",
    "lr_model = lr.fit(train_feature_vector_sdf)\n",
    "\n",
    "lr_predictions = lr_model.transform(test_feature_vector_sdf)\n",
    "lr_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf46396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|       rawPrediction|         probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[18.6086266693526...|[0.99997762791224...|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[18.8180066107263...|[0.99997581287298...|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[22.6963845270307...|[0.99999942608846...|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[16.7506644665745...|[0.99971232954776...|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[17.3393987944099...|[0.99969323067671...|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "정확도 1.0\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression 사용해 보기[실습]\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# ML 알고리즘 객체 생성\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='target', maxIter=10)\n",
    "\n",
    "lr_model = lr.fit(train_feature_vector_sdf)\n",
    "\n",
    "predictions = lr_model.transform(test_feature_vector_sdf)\n",
    "predictions.show(5)\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(\"정확도\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6ed06",
   "metadata": {},
   "source": [
    "# 파이프라인 구축\n",
    "- pipeline은 여러 개의 개별적인 Transformer의 변환 작업, Estimator의 학습작업을 일련의 프로세스 연결을 통해 간단한 API 처리로 구현할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "842b623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# Pipeline은 개별 변환 및 모델 학습 작업을 각각의 stage로 정의해서 파이프라인에 순서대로 등록\n",
    "# pipeline.fit()메소드를 활용해서 순서대로 연결된 스테이지 작업을 일괄적으로 수행\n",
    "# pipeline.fit()의 결과물은 pipelineModel로 반환이 된다.\n",
    "# pipelineModel에서 예측 작업을 transform()로 수행\n",
    "\n",
    "# 첫 번째 stage는 Feature Vectorization을 위한 VectorAssembler\n",
    "stage_1 = VectorAssembler(inputCols=iris_columns, outputCol=\"features\")\n",
    "\n",
    "# 두 번째 stage는 학습을 위한 모델을 생성\n",
    "stage_2 = DecisionTreeClassifier(featuresCol=\"features\",labelCol=\"target\",maxDepth=3)\n",
    "\n",
    "# 리스트를 활용해 stage를 순서대로 배치\n",
    "stages = [stage_1, stage_2]\n",
    "\n",
    "# 파이프라인에 등록\n",
    "pipeline = Pipeline(stages=stages)\n",
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6731a4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(train_sdf)\n",
    "type(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bb682c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|  probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인을 통해서 테스트 예측\n",
    "predictions = pipeline_model.transform(test_sdf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5190d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda2086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e35aa464e4f518e3bd80b827b0a7a82608bf016d4c837622545242b48b9bfee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
